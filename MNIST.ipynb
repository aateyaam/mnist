{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>  Part - (1) : Develop a neural network based classification network from scratch: </b>  This programming assignment uses MNIST digit dataset. It consists of large collection of handwritten digits from 0 to 9. These images are formated as 28x28 pixel gray scale images. The objective of this programming assignment is to design a neural network architecture that takes input as 28x28 image (or 784 dimensional vector) as input and predicts the digit information in it. Although there are diffrent varieties of neural network architecture to solve this task, this programming assignment uses only the feed forward network.  \n",
        "\n",
        "<dt> <h6> 1. Load MNIST data and create train, test splits </dt> </h6>\n",
        "\n",
        "<dd> <h6> - The MNIST dataset consists of around 70,000 images. Divide the dataset into two segments: training and testing. Allocate 60,000 images for training and 10,000 images for testing\n",
        "</dd> </h6>\n",
        "<dd> <h6> - Code for downloading the data and creating train-test splits is provided </dd> </h6>\n",
        "\n",
        "<dt> <h6> 2. Design a simple classification network </dt> </h6>\n",
        "\n",
        "<dd> <h6> - Let us use three layer feed-forward neral network. Use 512 nodes in the hidden layers and 10 nodes in the output layer. The output $\\textbf{y}$ from the input $\\textbf{x}$ is computed as follows </dd> </h6>\n",
        "<dd> <h6> <center> $ \\textbf{y} = h(\\textbf{W}_{3}g(\\textbf{W}_{2}g(\\textbf{W}_{1}\\textbf{x}))) $ </center> </dd> </h6>\n",
        "<dd> <h6> where $\\textbf{W}_{1} \\in \\mathcal{R}^{512 \\times 768}$,$\\textbf{W}_{2} \\in \\mathcal{R}^{512 \\times 512}$,$\\textbf{W}_{3} \\in \\mathcal{R}^{10 \\times 512} $ are the parameters of the network. g(.) is the hidden layer activation function. h(.) is the output layer activation function   </dd> </h6>\n",
        "<dd> <h6> - Consider g(.) as ReLU activation function. Softmax activation function should be used at the last layer h(.), to get the posterior probability of the classes. </dd> </h6>\n",
        "\n",
        "<dt> <h6> - Training classification network: </dt> </h6>\n",
        "\n",
        "<dd> <h6>  - Flatten the 28x28 images to arrive at 784 dimensional vector.  </dd> </h6>  \n",
        "<dd> <h6> - Randomly initialize the parameters of network, $\\textbf{W}_{1} \\in \\mathcal{R}^{768 \\times 512}$,$\\textbf{W}_{2} \\in \\mathcal{R}^{512 \\times 512}$,$\\textbf{W}_{3} \\in \\mathcal{R}^{512 \\times 10}$  </dd> </h6>\n",
        "<dd> <h6> - Feedforward the batch of input vectors to get the posterior probability of classes.  </dd> </h6>\n",
        "<dd> <h6> - Compute the loss between the estimated posterior probabilities and the true targets. </dd> </h6>  \n",
        "<dd> <h6> - Update the parameters of network to minimize the loss function.  </dd> </h6>\n",
        "<dd> <h6> <dd> <h6> - Backpropagate the loss function to get the gradients.  </dd> </h6> </dd> </h6>  \n",
        "\n",
        "<dd> <h6> <dd> <h6> - You can use stochastic gradient descent (SGD) optimization algorithm to update the parameters.  </dd> </h6> </dd> </h6>  \n",
        "<dd> <h6> <dd> <h6> - Cleverly set the hyperparameters involved in this optimization process. </dd> </h6> </dd> </h6>\n",
        "\n",
        "<dt> <h6> 3. Evaluate the performance of classification network </dd> </h6>\n",
        "<dd> <h6> - feed-forward the MNIST data through the trained classification network to get class posteriors. </dd> </h6>  \n",
        "<dd> <h6> - Assign the input to the class having maximum posterior probability </dd> </h6>  \n",
        "<dd> <h6> - Compute the loss and accuaracy </dd> </h6>  \n",
        "<dd> <h6> - Report your observations </dd> </h6>  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LYAp-ibURvdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "63TGxyPNSSEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Load MNIST data.\n",
        "##################################################\n",
        "import torchvision.datasets as datasets\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
        "\n",
        "#Training data\n",
        "mnist_traindata = mnist_trainset.data.numpy()\n",
        "mnist_trainlabel = mnist_trainset.targets.numpy()\n",
        "print(\"Training data\",mnist_traindata.shape)\n",
        "print(\"Training labels\",mnist_trainlabel.shape)\n",
        "\n",
        "#Testing data\n",
        "mnist_testdata = mnist_testset.data.numpy()\n",
        "mnist_testlabel = mnist_testset.targets.numpy()\n",
        "print(\"Testing data\",mnist_testdata.shape)\n",
        "print(\"Testing labels\",mnist_testlabel.shape)"
      ],
      "metadata": {
        "id": "eEbdtJuATJKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28bdfb0-6593-47ba-9495-35499ec80053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 3806128.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 612179.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5568695.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2564001.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Training data (60000, 28, 28)\n",
            "Training labels (60000,)\n",
            "Testing data (10000, 28, 28)\n",
            "Testing labels (10000,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Define the architecture\n",
        "##################################################\n",
        "\n",
        "#Complete the below function to impliment ReLU activation function\n",
        "def ReLu(inp):\n",
        "  return np.maximum(0, inp)\n",
        "\n",
        "#Complete the below function to impliment gradient of ReLU activation function\n",
        "def gradReLu(inp):\n",
        "  return np.where(inp>0,1,0)\n",
        "\n",
        "\n",
        "def softmax(inp):\n",
        "    exp_inp = np.exp(inp - np.max(inp, axis=1, keepdims=True))\n",
        "    return exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def compute_loss(y_out, targets):\n",
        "    return -np.mean(np.sum(targets * np.log(y_out + 1e-9), axis=1))\n",
        "\n",
        "\n",
        "\n",
        "#Complete the below function to impliment forward propagation of data\n",
        "def fwdPropagate(inputs, weights,bias,activation_fun ):\n",
        "  #Inputs: input data, paramters of network\n",
        "  w1, w2, w3 = weights\n",
        "  b1,b2,b3 = bias\n",
        "  z1 = np.dot(inputs,w1.T)+b1\n",
        "  a1 = activation_fun(z1)\n",
        "\n",
        "  z2 = np.dot(a1,w2.T)+b2\n",
        "  a2 = activation_fun(z2)\n",
        "\n",
        "  z3 = np.dot(a2,w3.T)+b3\n",
        "  targets = softmax(z3)\n",
        "\n",
        "  outps = (targets, z1,z2,z3)\n",
        "\n",
        "  #Return the requires outputs, i.e., final output and intermediate activations\n",
        "  return outps\n",
        "\n",
        "#Complete the below function to compute the gradients\n",
        "def computeGradients(inputs, targets, weights,bias, activations,activation_fun,grad_fun):\n",
        "  #Inputs: input data, targets, parameters of netwrok, intermediate activations\n",
        "  w1,w2,w3=weights\n",
        "  b1,b2,b3=bias\n",
        "  y_out, z1, z2,z3 = activations\n",
        "  sample_size=inputs.shape[0]\n",
        "  a1 = activation_fun(z1)\n",
        "  a2 = activation_fun(z2)\n",
        "  m=len(inputs)\n",
        "\n",
        "  dL_dw3=np.zeros_like(w3)\n",
        "  dL_dw2=np.zeros_like(w2)\n",
        "  dL_dw1=np.zeros_like(w1)\n",
        "  dL_db3=np.zeros_like(b3)\n",
        "  dL_db2=np.zeros_like(b2)\n",
        "  dL_db1=np.zeros_like(b1)\n",
        "\n",
        "  if (m>0):\n",
        "    delta3 = (y_out-targets)\n",
        "    delta2 = np.dot(delta3,w3)*grad_fun(z2)\n",
        "    delta1=np.dot(delta2,w2)*grad_fun(z1)\n",
        "\n",
        "    dL_dw3=np.dot(delta3.T,a2)/sample_size\n",
        "    dL_dw2=np.dot(delta2.T,a1)/sample_size\n",
        "    dL_dw1=np.dot(delta1.T,inputs)/sample_size\n",
        "\n",
        "    dL_db3 = np.sum(delta3,axis=0)/sample_size\n",
        "    dL_db2 = np.sum(delta2,axis=0)/sample_size\n",
        "    dL_db1 = np.sum(delta1,axis=0)/sample_size\n",
        "\n",
        "  #Compute the loss\n",
        "  loss = compute_loss(y_out,targets)\n",
        "  #Compote the derivative of loss at parameters\n",
        "  gradients_w=[dL_dw1, dL_dw2, dL_dw3]\n",
        "  gradients_b=[dL_db1, dL_db2, dL_db3]\n",
        "  #Return the gradients\n",
        "  return gradients_w,gradients_b,loss\n",
        "\n",
        "\n",
        "\n",
        "#Complete the below function to update the parameters using the above computed gradients\n",
        "def applyGradients(weights, bias,gradients_w, gradients_b, learning_rate):\n",
        "  #Inputs: weights, gradients, and learning rate\n",
        "  w1, w2, w3 = weights\n",
        "  b1,b2,b3=bias\n",
        "  [dL_dw1, dL_dw2, dL_dw3] = gradients_w\n",
        "  [dL_db1, dL_db2, dL_db3] = gradients_b\n",
        "  w1 = w1-learning_rate * dL_dw1\n",
        "  w2 = w2 - learning_rate * dL_dw2\n",
        "  w3 = w3 - learning_rate* dL_dw3\n",
        "\n",
        "  b1 = b1-learning_rate * dL_db1\n",
        "  b2 = b2 - learning_rate*dL_db2\n",
        "  b3 = b3 - learning_rate*dL_db3\n",
        "\n",
        "  weights=[w1,w2,w3]\n",
        "  bias=[b1,b2,b3]\n",
        "  #Return the updated parameters\n",
        "  return weights,bias\n",
        "\n",
        "#Complete the below function to complete the backpropagation step\n",
        "def backPropagate(inputs, targets, weights,bias, activations, learning_rate,activation_fun,grad_fun):\n",
        "  #Inputs: input data, targets, parameters of network, intermediate activations, learning rate of optimization algorithm\n",
        "  w1, w2, w3 = weights\n",
        "  b1 ,b2, b3 =  bias\n",
        "  y_out , z1 ,z2 , z3 = activations\n",
        "\n",
        "  #Compute the gradients\n",
        "  gradients_w, gradients_b,loss = computeGradients(inputs, targets, weights,bias, activations,activation_fun,grad_fun)\n",
        "\n",
        "  #Update the paramters using gradients\n",
        "  weights,bias=applyGradients(weights, bias,gradients_w, gradients_b, learning_rate)\n",
        "\n",
        "  #Return the updated parameters\n",
        "  return weights,bias,loss\n",
        "\n",
        "def compute_accuracy(y_out,targets):\n",
        "  pred_labels=np.argmax(y_out,axis=1)\n",
        "  true_labels=np.argmax(targets,axis=1)\n",
        "  accuracy=np.mean(pred_labels==true_labels)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Train the network\n",
        "##################################################\n",
        "\n",
        "#Complete the below function to complete the training of network\n",
        "def training(inputs, targets_idx,activation_fun,grad_fun, batch_size = 128, epochs=30, train_val_split=0.9,learning_rate=0.00001):\n",
        "  #Set the hyperparameters\n",
        "  hidden_units =512\n",
        "  n_classes =10\n",
        "  n_samples =inputs.shape[0]\n",
        "  n_batches =n_samples//batch_size\n",
        "\n",
        "  #Split the training data into two parts.\n",
        "  #Use 90 percent of training data for training the network.\n",
        "  #Remaining 10 percent as validation data\n",
        "  inputs = inputs.astype('float32') / 255.0\n",
        "  inputs=inputs.reshape(inputs.shape[0],-1)\n",
        "  index = int(train_val_split*n_samples)\n",
        "  training_data = inputs[:index]\n",
        "  training_targets = targets_idx[:index]\n",
        "  validation_data = inputs[index:]\n",
        "  validation_targets = targets_idx[index:]\n",
        "\n",
        "  #Randomly initialize the weights\n",
        "  W1 = np.random.randn(hidden_units,inputs.shape[1])*np.sqrt(2. / inputs.shape[1])\n",
        "  W2 = np.random.randn(hidden_units, hidden_units)*np.sqrt(2. / hidden_units)\n",
        "  W3 = np.random.randn(n_classes,hidden_units)*np.sqrt(2. / hidden_units)\n",
        "  weights = [W1,W2,W3]\n",
        "  b1=np.zeros((1,hidden_units))\n",
        "  b2=np.zeros((1,hidden_units))\n",
        "  b3=np.zeros((1,n_classes))\n",
        "\n",
        "  bias=[b1,b2,b3]\n",
        "  training_targets=np.eye(n_classes)[training_targets]\n",
        "  validation_targets=np.eye(n_classes)[validation_targets]\n",
        "  #Interate for epochs times\n",
        "  for epoch in range(epochs):\n",
        "    #Shuffle the training data\n",
        "    shuffle_indices = np.random.permutation(training_data.shape[0])\n",
        "    training_data_shuffled = training_data[shuffle_indices]\n",
        "    training_targets_shuffled = training_targets[shuffle_indices]\n",
        "    epoch_loss=0\n",
        "    #Interate through the batches of data\n",
        "    for batch in range(n_batches):\n",
        "      #Get the batch of data\n",
        "      start_idx = batch * batch_size\n",
        "      end_idx = (batch + 1) * batch_size\n",
        "      if end_idx > training_data_shuffled.shape[0]:\n",
        "        end_idx = training_data_shuffled.shape[0]\n",
        "      batch_inputs = training_data_shuffled[start_idx:end_idx]\n",
        "      batch_targets = training_targets_shuffled[start_idx:end_idx]\n",
        "      if batch_inputs.size == 0 or batch_targets.size == 0:\n",
        "        continue\n",
        "      #Forward propagation\n",
        "      activations = fwdPropagate(batch_inputs, weights,bias,activation_fun)\n",
        "\n",
        "      #Backward propagation\n",
        "      weights,bias,loss = backPropagate(batch_inputs, batch_targets, weights,bias, activations, learning_rate,activation_fun,grad_fun)\n",
        "      epoch_loss+=loss\n",
        "\n",
        "    #Compute outpus on trianing data\n",
        "    train_activations=fwdPropagate(training_data_shuffled, weights,bias,activation_fun)\n",
        "    train_predictions=train_activations[0]\n",
        "    train_accuracy = compute_accuracy(train_predictions,training_targets_shuffled)\n",
        "    train_loss=compute_loss(train_predictions,training_targets_shuffled)\n",
        "\n",
        "    #Print the statistics of training, i.e., training error, training accuracy, validation error, and validation accuracy\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss:{epoch_loss:.4f}')\n",
        "    print(f'Train_loss: {train_loss:.4f},Train_accuracy:{train_accuracy:.4f}')\n",
        "\n",
        "  #Compute outputs on validation data\n",
        "  val_activations=fwdPropagate(validation_data, weights,bias,activation_fun)\n",
        "  val_predictions=val_activations[0]\n",
        "  val_accuracy = compute_accuracy(val_predictions,validation_targets)\n",
        "  val_loss=compute_loss(val_predictions,validation_targets)\n",
        "  print(f'Val_loss:{val_loss:.4f},Val_accuracy:{val_accuracy:.4f}')\n",
        "\n",
        "  #Save the parameters of network\n",
        "  return weights,bias\n",
        "\n",
        "\n",
        "#Call the training function to train the network\n",
        "activation_fun=ReLu\n",
        "grad_fun=gradReLu\n",
        "weights,bias=training(mnist_traindata, mnist_trainlabel,activation_fun,grad_fun, batch_size = 128, epochs=30, train_val_split=0.9, learning_rate=0.001)\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on test data\n",
        "##################################################\n",
        "mnist_testdata = mnist_testdata.astype('float32') / 255.0\n",
        "mnist_testdata = mnist_testdata.reshape(mnist_testdata.shape[0], -1)\n",
        "mnist_testlabel=np.eye(10)[mnist_testlabel]\n",
        "test_activations=fwdPropagate(mnist_testdata, weights,bias,activation_fun)\n",
        "test_predictions=test_activations[0]\n",
        "test_accuracy = compute_accuracy(test_predictions,mnist_testlabel)\n",
        "test_loss=compute_loss(test_predictions,mnist_testlabel)\n",
        "print(f'Test loss: {test_loss:.4f},test_accuracy:{test_accuracy:.4f}')\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on MNIST test data\n",
        "##################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "q_xQY5DRV1WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ff4706-1046-4d8b-81ce-3409c9238cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss:873.9687\n",
            "Train_loss: 1.7947,Train_accuracy:0.5757\n",
            "Epoch 2/30, Loss:662.9999\n",
            "Train_loss: 1.3675,Train_accuracy:0.7332\n",
            "Epoch 3/30, Loss:510.6150\n",
            "Train_loss: 1.0719,Train_accuracy:0.7856\n",
            "Epoch 4/30, Loss:409.2898\n",
            "Train_loss: 0.8811,Train_accuracy:0.8195\n",
            "Epoch 5/30, Loss:344.0145\n",
            "Train_loss: 0.7568,Train_accuracy:0.8357\n",
            "Epoch 6/30, Loss:300.6252\n",
            "Train_loss: 0.6721,Train_accuracy:0.8483\n",
            "Epoch 7/30, Loss:270.3398\n",
            "Train_loss: 0.6114,Train_accuracy:0.8569\n",
            "Epoch 8/30, Loss:248.1342\n",
            "Train_loss: 0.5659,Train_accuracy:0.8633\n",
            "Epoch 9/30, Loss:231.1732\n",
            "Train_loss: 0.5306,Train_accuracy:0.8683\n",
            "Epoch 10/30, Loss:217.8435\n",
            "Train_loss: 0.5021,Train_accuracy:0.8738\n",
            "Epoch 11/30, Loss:206.9909\n",
            "Train_loss: 0.4789,Train_accuracy:0.8777\n",
            "Epoch 12/30, Loss:198.0079\n",
            "Train_loss: 0.4595,Train_accuracy:0.8814\n",
            "Epoch 13/30, Loss:190.4337\n",
            "Train_loss: 0.4430,Train_accuracy:0.8848\n",
            "Epoch 14/30, Loss:183.9449\n",
            "Train_loss: 0.4285,Train_accuracy:0.8877\n",
            "Epoch 15/30, Loss:178.2548\n",
            "Train_loss: 0.4160,Train_accuracy:0.8898\n",
            "Epoch 16/30, Loss:173.3140\n",
            "Train_loss: 0.4048,Train_accuracy:0.8923\n",
            "Epoch 17/30, Loss:168.8645\n",
            "Train_loss: 0.3950,Train_accuracy:0.8939\n",
            "Epoch 18/30, Loss:164.9436\n",
            "Train_loss: 0.3861,Train_accuracy:0.8960\n",
            "Epoch 19/30, Loss:161.3547\n",
            "Train_loss: 0.3781,Train_accuracy:0.8981\n",
            "Epoch 20/30, Loss:158.0946\n",
            "Train_loss: 0.3707,Train_accuracy:0.8995\n",
            "Epoch 21/30, Loss:155.1171\n",
            "Train_loss: 0.3639,Train_accuracy:0.9012\n",
            "Epoch 22/30, Loss:152.3674\n",
            "Train_loss: 0.3577,Train_accuracy:0.9027\n",
            "Epoch 23/30, Loss:149.8405\n",
            "Train_loss: 0.3519,Train_accuracy:0.9043\n",
            "Epoch 24/30, Loss:147.4519\n",
            "Train_loss: 0.3465,Train_accuracy:0.9051\n",
            "Epoch 25/30, Loss:145.2612\n",
            "Train_loss: 0.3414,Train_accuracy:0.9060\n",
            "Epoch 26/30, Loss:143.2300\n",
            "Train_loss: 0.3366,Train_accuracy:0.9071\n",
            "Epoch 27/30, Loss:141.2759\n",
            "Train_loss: 0.3322,Train_accuracy:0.9083\n",
            "Epoch 28/30, Loss:139.4289\n",
            "Train_loss: 0.3279,Train_accuracy:0.9089\n",
            "Epoch 29/30, Loss:137.6508\n",
            "Train_loss: 0.3240,Train_accuracy:0.9101\n",
            "Epoch 30/30, Loss:136.0141\n",
            "Train_loss: 0.3201,Train_accuracy:0.9110\n",
            "Val_loss:0.2621,Val_accuracy:0.9275\n",
            "Test loss: 0.3021,test_accuracy:0.9145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (2) : Understanding activation functions: </b> In this part you will learn to use different activation functions for the classification task and compare their performances.\n",
        "\n",
        "<dt> <h6> 1. Train MNIST digit classification network with different activation functions i.e. Sigmoid, Tanh, ReLU, LeakyReLU etc. You can stick to stochastic gradient descent optimization algorithm for this part </dt> </h6>\n",
        "<dt> <h6> 2. Report the accuray on MNIST test data for all the experiments. Write down your observations in the report.</br> </dt> </h6>\n",
        "\n"
      ],
      "metadata": {
        "id": "7AUF8zcrO1kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Train the network with different activation functions\n",
        "##################################################\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "def ReLu(x):\n",
        "  return np.max(0,x)\n",
        "\n",
        "def LeakyReLu(x):\n",
        "  return np.maximum(0.1*x,x)\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  return np.exp(-x)/(1+np.exp(-x))**2\n",
        "\n",
        "def gradReLu(x):\n",
        "  return np.where(x>0,1,0)\n",
        "\n",
        "def grad_tanh(x):\n",
        "  return 4*(1/(np.exp(x)+np.exp(-x))**2)\n",
        "\n",
        "def grad_LeakyReLu(x):\n",
        "  return np.where(x>0,1,0.1)\n",
        "\n",
        "\n",
        "###### SIGMOID\n",
        "print(\"SIGMOID ACTIVATION FUNCTION\")\n",
        "activation_fun=sigmoid\n",
        "grad_fun=grad_sigmoid\n",
        "#Call the training function to train the network\n",
        "weights,bias=training(mnist_traindata, mnist_trainlabel, activation_fun,grad_fun,batch_size = 128, epochs=30, train_val_split=0.9, learning_rate=0.001)\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on test data\n",
        "##################################################\n",
        "mnist_testdata = mnist_testdata.reshape(mnist_testdata.shape[0], -1)\n",
        "# mnist_testlabel=np.eye(10)[mnist_testlabel]\n",
        "test_activations=fwdPropagate(mnist_testdata, weights,bias,activation_fun)\n",
        "test_predictions=test_activations[0]\n",
        "test_accuracy = compute_accuracy(test_predictions,mnist_testlabel)\n",
        "test_loss=compute_loss(test_predictions,mnist_testlabel)\n",
        "print(f'Test loss: {test_loss:.4f},test_accuracy:{test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "###### LeakyReLu\n",
        "print(\"LEAKY RELU ACTIVATION FUNCTION\")\n",
        "activation_fun=LeakyReLu\n",
        "grad_fun=grad_LeakyReLu\n",
        "#Call the training function to train the network\n",
        "weights,bias=training(mnist_traindata, mnist_trainlabel, activation_fun,grad_fun,batch_size = 128, epochs=30, train_val_split=0.9, learning_rate=0.001)\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on test data\n",
        "##################################################\n",
        "mnist_testdata = mnist_testdata.reshape(mnist_testdata.shape[0], -1)\n",
        "# mnist_testlabel=np.eye(10)[mnist_testlabel]\n",
        "test_activations=fwdPropagate(mnist_testdata, weights,bias,activation_fun)\n",
        "test_predictions=test_activations[0]\n",
        "test_accuracy = compute_accuracy(test_predictions,mnist_testlabel)\n",
        "test_loss=compute_loss(test_predictions,mnist_testlabel)\n",
        "print(f'Test loss: {test_loss:.4f},test_accuracy:{test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "###### tanh\n",
        "print(\"TANH ACTIVATION FUNCTION\")\n",
        "activation_fun=tanh\n",
        "grad_fun=grad_tanh\n",
        "#Call the training function to train the network\n",
        "weights,bias=training(mnist_traindata, mnist_trainlabel, activation_fun,grad_fun,batch_size = 128, epochs=30, train_val_split=0.9, learning_rate=0.001)\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on test data\n",
        "##################################################\n",
        "mnist_testdata = mnist_testdata.reshape(mnist_testdata.shape[0], -1)\n",
        "# mnist_testlabel=np.eye(10)[mnist_testlabel]\n",
        "test_activations=fwdPropagate(mnist_testdata, weights,bias,activation_fun)\n",
        "test_predictions=test_activations[0]\n",
        "test_accuracy = compute_accuracy(test_predictions,mnist_testlabel)\n",
        "test_loss=compute_loss(test_predictions,mnist_testlabel)\n",
        "print(f'Test loss: {test_loss:.4f},test_accuracy:{test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4HNrJP_1UyDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a43bd68-0427-429d-8c96-201abbfae899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIGMOID ACTIVATION FUNCTION\n",
            "Epoch 1/30, Loss:973.4544\n",
            "Train_loss: 2.2831,Train_accuracy:0.1961\n",
            "Epoch 2/30, Loss:960.8836\n",
            "Train_loss: 2.2708,Train_accuracy:0.1827\n",
            "Epoch 3/30, Loss:955.7318\n",
            "Train_loss: 2.2583,Train_accuracy:0.2257\n",
            "Epoch 4/30, Loss:950.5197\n",
            "Train_loss: 2.2463,Train_accuracy:0.2600\n",
            "Epoch 5/30, Loss:945.3539\n",
            "Train_loss: 2.2339,Train_accuracy:0.3662\n",
            "Epoch 6/30, Loss:940.1236\n",
            "Train_loss: 2.2214,Train_accuracy:0.4029\n",
            "Epoch 7/30, Loss:934.8322\n",
            "Train_loss: 2.2089,Train_accuracy:0.4042\n",
            "Epoch 8/30, Loss:929.5280\n",
            "Train_loss: 2.1960,Train_accuracy:0.5039\n",
            "Epoch 9/30, Loss:924.0792\n",
            "Train_loss: 2.1831,Train_accuracy:0.5230\n",
            "Epoch 10/30, Loss:918.5311\n",
            "Train_loss: 2.1699,Train_accuracy:0.5011\n",
            "Epoch 11/30, Loss:912.9483\n",
            "Train_loss: 2.1564,Train_accuracy:0.5387\n",
            "Epoch 12/30, Loss:907.1633\n",
            "Train_loss: 2.1428,Train_accuracy:0.5637\n",
            "Epoch 13/30, Loss:901.3158\n",
            "Train_loss: 2.1286,Train_accuracy:0.5486\n",
            "Epoch 14/30, Loss:895.2908\n",
            "Train_loss: 2.1141,Train_accuracy:0.5339\n",
            "Epoch 15/30, Loss:889.1007\n",
            "Train_loss: 2.0992,Train_accuracy:0.5846\n",
            "Epoch 16/30, Loss:882.7564\n",
            "Train_loss: 2.0841,Train_accuracy:0.5995\n",
            "Epoch 17/30, Loss:876.2132\n",
            "Train_loss: 2.0683,Train_accuracy:0.6331\n",
            "Epoch 18/30, Loss:869.4865\n",
            "Train_loss: 2.0520,Train_accuracy:0.6224\n",
            "Epoch 19/30, Loss:862.5544\n",
            "Train_loss: 2.0354,Train_accuracy:0.6385\n",
            "Epoch 20/30, Loss:855.3817\n",
            "Train_loss: 2.0182,Train_accuracy:0.6089\n",
            "Epoch 21/30, Loss:848.0102\n",
            "Train_loss: 2.0004,Train_accuracy:0.6183\n",
            "Epoch 22/30, Loss:840.3897\n",
            "Train_loss: 1.9820,Train_accuracy:0.6201\n",
            "Epoch 23/30, Loss:832.5491\n",
            "Train_loss: 1.9631,Train_accuracy:0.6549\n",
            "Epoch 24/30, Loss:824.4435\n",
            "Train_loss: 1.9437,Train_accuracy:0.6408\n",
            "Epoch 25/30, Loss:816.0815\n",
            "Train_loss: 1.9236,Train_accuracy:0.6586\n",
            "Epoch 26/30, Loss:807.5000\n",
            "Train_loss: 1.9028,Train_accuracy:0.6601\n",
            "Epoch 27/30, Loss:798.6791\n",
            "Train_loss: 1.8817,Train_accuracy:0.6622\n",
            "Epoch 28/30, Loss:789.5845\n",
            "Train_loss: 1.8598,Train_accuracy:0.6629\n",
            "Epoch 29/30, Loss:780.2420\n",
            "Train_loss: 1.8375,Train_accuracy:0.6703\n",
            "Epoch 30/30, Loss:770.6664\n",
            "Train_loss: 1.8146,Train_accuracy:0.6815\n",
            "Val_loss:1.7974,Val_accuracy:0.7162\n",
            "Test loss: 1.8047,test_accuracy:0.6972\n",
            "LEAKY RELU ACTIVATION FUNCTION\n",
            "Epoch 1/30, Loss:849.3668\n",
            "Train_loss: 1.7120,Train_accuracy:0.6230\n",
            "Epoch 2/30, Loss:627.4769\n",
            "Train_loss: 1.2885,Train_accuracy:0.7422\n",
            "Epoch 3/30, Loss:482.2753\n",
            "Train_loss: 1.0170,Train_accuracy:0.7907\n",
            "Epoch 4/30, Loss:390.8104\n",
            "Train_loss: 0.8469,Train_accuracy:0.8169\n",
            "Epoch 5/30, Loss:332.6550\n",
            "Train_loss: 0.7359,Train_accuracy:0.8334\n",
            "Epoch 6/30, Loss:293.6328\n",
            "Train_loss: 0.6592,Train_accuracy:0.8455\n",
            "Epoch 7/30, Loss:265.9510\n",
            "Train_loss: 0.6033,Train_accuracy:0.8554\n",
            "Epoch 8/30, Loss:245.4023\n",
            "Train_loss: 0.5608,Train_accuracy:0.8621\n",
            "Epoch 9/30, Loss:229.4914\n",
            "Train_loss: 0.5274,Train_accuracy:0.8689\n",
            "Epoch 10/30, Loss:216.8247\n",
            "Train_loss: 0.5005,Train_accuracy:0.8734\n",
            "Epoch 11/30, Loss:206.5101\n",
            "Train_loss: 0.4782,Train_accuracy:0.8769\n",
            "Epoch 12/30, Loss:197.9064\n",
            "Train_loss: 0.4595,Train_accuracy:0.8810\n",
            "Epoch 13/30, Loss:190.5827\n",
            "Train_loss: 0.4435,Train_accuracy:0.8846\n",
            "Epoch 14/30, Loss:184.3158\n",
            "Train_loss: 0.4297,Train_accuracy:0.8872\n",
            "Epoch 15/30, Loss:178.8577\n",
            "Train_loss: 0.4175,Train_accuracy:0.8896\n",
            "Epoch 16/30, Loss:174.0183\n",
            "Train_loss: 0.4068,Train_accuracy:0.8916\n",
            "Epoch 17/30, Loss:169.7319\n",
            "Train_loss: 0.3972,Train_accuracy:0.8929\n",
            "Epoch 18/30, Loss:165.8670\n",
            "Train_loss: 0.3888,Train_accuracy:0.8953\n",
            "Epoch 19/30, Loss:162.4313\n",
            "Train_loss: 0.3807,Train_accuracy:0.8964\n",
            "Epoch 20/30, Loss:159.2779\n",
            "Train_loss: 0.3736,Train_accuracy:0.8981\n",
            "Epoch 21/30, Loss:156.4007\n",
            "Train_loss: 0.3671,Train_accuracy:0.8993\n",
            "Epoch 22/30, Loss:153.7418\n",
            "Train_loss: 0.3611,Train_accuracy:0.9008\n",
            "Epoch 23/30, Loss:151.2761\n",
            "Train_loss: 0.3553,Train_accuracy:0.9019\n",
            "Epoch 24/30, Loss:148.9723\n",
            "Train_loss: 0.3502,Train_accuracy:0.9032\n",
            "Epoch 25/30, Loss:146.8347\n",
            "Train_loss: 0.3452,Train_accuracy:0.9047\n",
            "Epoch 26/30, Loss:144.8567\n",
            "Train_loss: 0.3406,Train_accuracy:0.9056\n",
            "Epoch 27/30, Loss:142.9626\n",
            "Train_loss: 0.3363,Train_accuracy:0.9068\n",
            "Epoch 28/30, Loss:141.1895\n",
            "Train_loss: 0.3322,Train_accuracy:0.9080\n",
            "Epoch 29/30, Loss:139.4747\n",
            "Train_loss: 0.3285,Train_accuracy:0.9093\n",
            "Epoch 30/30, Loss:137.8974\n",
            "Train_loss: 0.3247,Train_accuracy:0.9099\n",
            "Val_loss:0.2676,Val_accuracy:0.9268\n",
            "Test loss: 0.3064,test_accuracy:0.9128\n",
            "TANH ACTIVATION FUNCTION\n",
            "Epoch 1/30, Loss:713.5857\n",
            "Train_loss: 1.2627,Train_accuracy:0.7029\n",
            "Epoch 2/30, Loss:449.4035\n",
            "Train_loss: 0.9177,Train_accuracy:0.7859\n",
            "Epoch 3/30, Loss:350.5799\n",
            "Train_loss: 0.7592,Train_accuracy:0.8182\n",
            "Epoch 4/30, Loss:299.8305\n",
            "Train_loss: 0.6678,Train_accuracy:0.8350\n",
            "Epoch 5/30, Loss:268.5089\n",
            "Train_loss: 0.6078,Train_accuracy:0.8447\n",
            "Epoch 6/30, Loss:247.0832\n",
            "Train_loss: 0.5647,Train_accuracy:0.8533\n",
            "Epoch 7/30, Loss:231.2873\n",
            "Train_loss: 0.5321,Train_accuracy:0.8601\n",
            "Epoch 8/30, Loss:219.0960\n",
            "Train_loss: 0.5066,Train_accuracy:0.8659\n",
            "Epoch 9/30, Loss:209.3513\n",
            "Train_loss: 0.4857,Train_accuracy:0.8700\n",
            "Epoch 10/30, Loss:201.3356\n",
            "Train_loss: 0.4684,Train_accuracy:0.8731\n",
            "Epoch 11/30, Loss:194.6323\n",
            "Train_loss: 0.4538,Train_accuracy:0.8762\n",
            "Epoch 12/30, Loss:188.8852\n",
            "Train_loss: 0.4412,Train_accuracy:0.8796\n",
            "Epoch 13/30, Loss:183.9299\n",
            "Train_loss: 0.4301,Train_accuracy:0.8816\n",
            "Epoch 14/30, Loss:179.5596\n",
            "Train_loss: 0.4205,Train_accuracy:0.8837\n",
            "Epoch 15/30, Loss:175.7090\n",
            "Train_loss: 0.4118,Train_accuracy:0.8859\n",
            "Epoch 16/30, Loss:172.2468\n",
            "Train_loss: 0.4040,Train_accuracy:0.8876\n",
            "Epoch 17/30, Loss:169.1379\n",
            "Train_loss: 0.3971,Train_accuracy:0.8890\n",
            "Epoch 18/30, Loss:166.3320\n",
            "Train_loss: 0.3907,Train_accuracy:0.8905\n",
            "Epoch 19/30, Loss:163.7441\n",
            "Train_loss: 0.3848,Train_accuracy:0.8919\n",
            "Epoch 20/30, Loss:161.3597\n",
            "Train_loss: 0.3794,Train_accuracy:0.8932\n",
            "Epoch 21/30, Loss:159.1546\n",
            "Train_loss: 0.3744,Train_accuracy:0.8945\n",
            "Epoch 22/30, Loss:157.1078\n",
            "Train_loss: 0.3698,Train_accuracy:0.8956\n",
            "Epoch 23/30, Loss:155.2302\n",
            "Train_loss: 0.3654,Train_accuracy:0.8967\n",
            "Epoch 24/30, Loss:153.4599\n",
            "Train_loss: 0.3614,Train_accuracy:0.8976\n",
            "Epoch 25/30, Loss:151.8260\n",
            "Train_loss: 0.3576,Train_accuracy:0.8987\n",
            "Epoch 26/30, Loss:150.2461\n",
            "Train_loss: 0.3540,Train_accuracy:0.8997\n",
            "Epoch 27/30, Loss:148.7854\n",
            "Train_loss: 0.3506,Train_accuracy:0.9004\n",
            "Epoch 28/30, Loss:147.4102\n",
            "Train_loss: 0.3473,Train_accuracy:0.9013\n",
            "Epoch 29/30, Loss:146.0739\n",
            "Train_loss: 0.3443,Train_accuracy:0.9021\n",
            "Epoch 30/30, Loss:144.8079\n",
            "Train_loss: 0.3414,Train_accuracy:0.9029\n",
            "Val_loss:0.2804,Val_accuracy:0.9232\n",
            "Test loss: 0.3191,test_accuracy:0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Understanding regularization methods: </b> In this part of the assignment, you will learn about a few regularization techniques to reduce the overfitting problem. Using the above built network, include the following techniques to reduce the overfitting by retraining the network efficiently. Write down the accuracies for each case.\n",
        "<dt> <h6> 1. Weight regularization: Add regularization term to the classification los </dt> </h6>\n",
        "<dt> <h6> 2. Dropout with a probability of 0.2: Randomly drop the activation potentials of hidden neural with 0.2 probability. Disable the dropout layer in inference model. You can experiment with different dropout probabilities and report your observations.  </dt> </h6>\n",
        "<dt> <h6> 3. Early stopping: Stop the network training when it is started to overfitting to training data. </dt> </h6>\n"
      ],
      "metadata": {
        "id": "2FXj11mJO_Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################\n",
        "#Training with weight regularization\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Training with dropout strategy\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Training with early stopping criterion\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jbXfF_W4Vwyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}